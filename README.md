PROJECT DESCRIPTION:

This project is about creating an ETL pipeline which extracts the data from Youtube Data API and transforms it using PySpark and loads the data in AWS S3.

-> Utilized Apache Airflow in creating the dag for the ETL tasks
-> Transformed the data to JSON and utilized Kafka as data sink and stored the data in S3 for further analysis.

USED TECHNOLOGIES, LIBRARIES AND APIs:

Apache Airflow - For orchestration of pipeline
Youtube Data API - For extracting the data
PySpark - To process the extracted data
AWS S3 - For storing the data
PROJECT ARCHITECTURE:
![image](https://github.com/Swathi-Reddy1408/Etl_Pipeline_With_Airflow/assets/52827609/45933ea1-bed8-4a6e-b65b-55fc3ff1f8e9)

